{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42e57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4fdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df=pd.read_csv('../data/processed/Harmonized_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e226b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PATTERN = re.compile(\n",
    "    r\"(https?://\\S+|www\\.\\S+|\\bURL\\b|\\bLINK\\b|@url)\", flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "MENTION_PATTERN = re.compile(r\"@\\w+\")\n",
    "\n",
    "\n",
    "RT_PATTERN = re.compile(r\"(^|\\s)(RT\\s+@?\\w+)\", flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "MULTI_SPACE_PATTERN = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "# Arabic letter normalization\n",
    "\n",
    "\n",
    "ARABIC_NORMALIZATION_MAP = {\n",
    "    \"أ\": \"ا\",\n",
    "    \"إ\": \"ا\",\n",
    "    \"آ\": \"ا\",\n",
    "    \"ى\": \"ي\",\n",
    "    \"ؤ\": \"و\",\n",
    "    \"ئ\": \"ي\",\n",
    "    \"ة\": \"ه\",\n",
    "    \"ـ\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_arabic(text: str) -> str:\n",
    "    for src, tgt in ARABIC_NORMALIZATION_MAP.items():\n",
    "        text = text.replace(src, tgt)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_invalid_unicode(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    Remove badly decoded unicode characters while\n",
    "\n",
    "\n",
    "    keeping emojis and Arabic/Latin text.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return \"\".join(ch for ch in text if unicodedata.category(ch)[0] != \"C\")\n",
    "\n",
    "\n",
    "def preprocess_tweet(text: str) -> str:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    SOTA preprocessing for Arabic Twitter hate speech.\n",
    "\n",
    "\n",
    "    Compatible with MARBERTv2 and OSACT-style datasets.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Unicode cleanup\n",
    "\n",
    "    text = remove_invalid_unicode(text)\n",
    "\n",
    "    # 2. Normalize newlines and tabs\n",
    "\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "    # 3. Normalize URLs\n",
    "\n",
    "    text = URL_PATTERN.sub(\" URL \", text)\n",
    "\n",
    "    # 4. Normalize mentions\n",
    "\n",
    "    text = MENTION_PATTERN.sub(\"@USER\", text)\n",
    "\n",
    "    # 5. Normalize RT marker\n",
    "\n",
    "    text = RT_PATTERN.sub(\" RT @USER \", text)\n",
    "\n",
    "    # 6. Arabic orthographic normalization\n",
    "\n",
    "    text = normalize_arabic(text)\n",
    "\n",
    "    # 7. Remove diacritics\n",
    "\n",
    "    text = re.sub(r\"[\\u064B-\\u065F\\u0670]\", \"\", text)\n",
    "\n",
    "    # 8. Remove excessive punctuation repetition\n",
    "\n",
    "    text = re.sub(r\"([!؟?]){3,}\", r\"\\1\\1\", text)\n",
    "\n",
    "    # 9. Normalize whitespace\n",
    "\n",
    "    text = MULTI_SPACE_PATTERN.sub(\" \", text).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant_subtype(row):\n",
    "    if row[\"is_hate\"] == 0:\n",
    "        return \"NH\"\n",
    "\n",
    "    if row[\"OH\"] == 1:\n",
    "        return \"OH\"\n",
    "\n",
    "    if row[\"GH\"] == 1:\n",
    "        return \"GH\"\n",
    "\n",
    "    if row[\"RH\"] == 1:\n",
    "        return \"RH\"\n",
    "\n",
    "    return \"UNK\"\n",
    "\n",
    "\n",
    "clean_df[\"stratify_label\"] = clean_df.apply(dominant_subtype, axis=1)\n",
    "\n",
    "print(clean_df[\"stratify_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    clean_df, test_size=0.2, stratify=clean_df[\"stratify_label\"], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "dev_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"stratify_label\"], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet('../data/processed/train.parquet', index=False)\n",
    "dev_df.to_parquet('../data/processed/dev.parquet', index=False)\n",
    "test_df.to_parquet('../data/processed/test.parquet', index=False)\n",
    "\n",
    "print(\"✅ Data splits saved to data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
