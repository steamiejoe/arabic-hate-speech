{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca83af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "import copy\n",
    "import gc\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa18634",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('../data/processed/train.parquet')\n",
    "dev_df = pd.read_parquet('../data/processed/dev.parquet')\n",
    "# test_df = pd.read_parquet('../data/processed/test.parquet') # If needed\n",
    "\n",
    "print(f\"Loaded Train: {train_df.shape}, Dev: {dev_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"UBC-NLP/MARBERTv2\"\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device detected: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc5e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hate_only = train_df[train_df[\"is_hate\"] == 1].copy()\n",
    "dev_hate_only = dev_df[dev_df[\"is_hate\"] == 1].copy()\n",
    "\n",
    "le_cascade = LabelEncoder()\n",
    "le_cascade.fit(train_hate_only[\"stratify_label\"])\n",
    "\n",
    "print(\"Data Split Statistics:\")\n",
    "print(f\"Stage 1 Data Size: {len(train_df)} rows\")\n",
    "print(f\"Stage 2 Data Size: {len(train_hate_only)} rows\")\n",
    "print(f\"Stage 2 Classes: {le_cascade.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, label_col, label_encoder=None):\n",
    "        self.texts = df[\"text_clean\"].astype(str).to_numpy()\n",
    "        self.labels = df[label_col].to_numpy()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_encoder = label_encoder\n",
    "        self.is_binary = label_col == \"is_hate\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        raw_label = self.labels[item]\n",
    "\n",
    "        if self.is_binary:\n",
    "            label_tensor = torch.tensor(raw_label, dtype=torch.long)\n",
    "        else:\n",
    "            encoded_label = self.label_encoder.transform([raw_label])[0]\n",
    "            label_tensor = torch.tensor(encoded_label, dtype=torch.long)\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": label_tensor,\n",
    "        }\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage(stage_name, train_df, dev_df, label_col, le=None, epochs=3):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Starting Training: {stage_name}\")\n",
    "    print(f\"Target Column: {label_col}\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    num_labels = 2 if label_col == \"is_hate\" else len(le.classes_)\n",
    "\n",
    "    train_ds = CascadeDataset(train_df, tokenizer, MAX_LEN, label_col, le)\n",
    "    val_ds = CascadeDataset(dev_df, tokenizer, MAX_LEN, label_col, le)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=num_labels\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    best_f1 = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", leave=False)\n",
    "\n",
    "        for batch in pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        preds_all, labels_all = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "                preds_all.extend(preds)\n",
    "                labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_f1 = f1_score(labels_all, preds_all, average=\"macro\")\n",
    "        val_acc = accuracy_score(labels_all, preds_all)\n",
    "\n",
    "        history.append(\n",
    "            {\n",
    "                \"Epoch\": epoch + 1,\n",
    "                \"Train Loss\": avg_train_loss,\n",
    "                \"Val Loss\": avg_val_loss,\n",
    "                \"Val F1\": val_f1,\n",
    "                \"Val Acc\": val_acc,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Val Loss={avg_val_loss:.4f} | F1={val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"\\nBest F1 Score: {best_f1:.4f}\")\n",
    "    df_history = pd.DataFrame(history)\n",
    "    display(df_history)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model = train_stage(\n",
    "    stage_name=\"Stage 1: Binary Classifier\",\n",
    "    train_df=train_df,\n",
    "    dev_df=dev_df,\n",
    "    label_col=\"is_hate\",\n",
    "    le=None,\n",
    "    epochs=4,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c34b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = train_stage(\n",
    "    stage_name=\"Stage 2: Type Classifier\",\n",
    "    train_df=train_hate_only,\n",
    "    dev_df=dev_hate_only,\n",
    "    label_col=\"stratify_label\",\n",
    "    le=le_cascade,\n",
    "    epochs=6,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84c5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cascade_single(text, binary_model, multi_model, tokenizer, le_multi):\n",
    "    binary_model.eval()\n",
    "    multi_model.eval()\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        bin_outputs = binary_model(**inputs)\n",
    "        bin_prob = torch.sigmoid(bin_outputs.logits[:, 1]).item()\n",
    "\n",
    "        if bin_prob < 0.5:\n",
    "            return \"NH\"\n",
    "\n",
    "        multi_outputs = multi_model(**inputs)\n",
    "        multi_pred_idx = torch.argmax(multi_outputs.logits, dim=1).item()\n",
    "        final_label = le_multi.inverse_transform([multi_pred_idx])[0]\n",
    "\n",
    "        return final_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Cascade System on Validation Set\")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for _, row in tqdm(dev_df.iterrows(), total=len(dev_df), desc=\"Inference\"):\n",
    "    text = str(row[\"text_clean\"])\n",
    "    true_label = row[\"stratify_label\"]\n",
    "\n",
    "    prediction = predict_cascade_single(\n",
    "        text, binary_model, multi_model, tokenizer, le_cascade\n",
    "    )\n",
    "\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL CASCADE CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
